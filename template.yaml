---
theme: default # default || classic || dark
organization: "Max Planck Institute for Informatics and Saarland University"
# twitter: "@denkivvakame"
title: "Follow My Hold: Hand-Object Interaction Reconstruction through Geometric Guidance"
# journal: "AAAA'24"

resources:
  # paper: "https://openreview.net/"
  arxiv: "https://arxiv.org/pdf/2508.18213"
  code: "https://github.com/aidilayce/FollowMyHold"
  # video: "https://www.youtube.com/embed/onbnb_D1wC8?si=xJczUv716Lt5aO4l&start=1150"
  demo: "https://aidilayce.github.io/FollowMyHold-page/"
  # huggingface: "https://huggingface.co/"

description: >
  Project page for FollowMyHold.

image: "images/teaser.png"
url: "https://aidilayce.github.io/FollowMyHold-page/"
# speakerdeck: # speakerdeck slide ID

authors:
  - name: "Ayce Idil Aytekin"
    affiliation: [1]
    url: "https://people.mpi-inf.mpg.de/~aaytekin/"
  - name: "Helge Rhodin"
    affiliation: [2]
    url: "https://helge.rhodin.de/index.html"
  - name: "Rishabh Dabral"
    affiliation: [1]
    url: "https://rishabhdabral.github.io/"
  - name: "Christian Theobalt"
    affiliation: [1]
    url: "https://people.mpi-inf.mpg.de/~theobalt/"

affiliations:
  - "Max Planck Institute for Informatics and Saarland University"
  - "Bielefeld University"

bibtex: |
  @misc{aytekin2025followholdhandobjectinteraction,
      title={Follow My Hold: Hand-Object Interaction Reconstruction through Geometric Guidance}, 
      author={Ayce Idil Aytekin and Helge Rhodin and Rishabh Dabral and Christian Theobalt},
      year={2025},
      eprint={2508.18213},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2508.18213}, 
  }

teaser: "images/teaser.png"

abstract: 
  We propose a novel diffusion-based framework for reconstructing 3D geometry
  of hand-held objects from monocular RGB images by leveraging hand-object
  interaction as geometric guidance. Our method conditions a latent diffusion
  model on an inpainted object appearance and uses inference-time guidance
  to optimize the object reconstruction, while simultaneously ensuring
  plausible hand-object interactions. Unlike prior methods that rely on
  extensive post-processing or produce low-quality reconstructions, our
  approach directly generates high-quality object geometry during the diffusion
  process by introducing guidance with an optimization-in-the-loop design.
  Specifically, we guide the diffusion model by applying supervision to the
  velocity field while simultaneously optimizing the transformations of both
  the hand and the object being reconstructed. This optimization is driven by
  multi-modal geometric cues, including normal and depth alignment, silhouette
  consistency, and 2D keypoint reprojection. We further incorporate signed
  distance field supervision and enforce contact and non-intersection
  constraints to ensure physical plausibility of hand-object interaction.
  Our method yields accurate, robust and coherent reconstructions under
  occlusion while generalizing well to in-the-wild scenarios.

body:
  - title: ""
    text: |
      <div class="tldr">
        <span class="tldr-badge">TL;DR</span>
        We guide a latent diffusion model with multi-modal cues derived from the input hand-object image and several
        foundation models to reconstruct hand-object interactions in 3D.
      </div>
  - title: "Interactive Results"
    text: |
      <div class="uk-grid-large" uk-grid>
        <!-- LEFT: examples -->
        <div class="uk-width-1-2@m">
          <div class="uk-grid-small uk-child-width-1-2" uk-grid>
            <div>
              <a
                class="swap-model"
                href="#"
                data-model="models/wild/hoi_4.glb"
              >
                <img
                  src="models/wild/4_full_image_1.png"
                  class="uk-width-1-1"
                  alt="Example #4"
                >
                <div class="uk-text-meta">Camera</div>
              </a>
            </div>
            <div>
              <a
                class="swap-model"
                href="#"
                data-model="models/wild/hoi_15.glb"
              >
                <img
                  src="models/wild/15_full_image_1.png"
                  class="uk-width-1-1"
                  alt="Example #5"
                >
                <div class="uk-text-meta">Ice cream</div>
              </a>
            </div>
            <div>
              <a
                class="swap-model"
                href="#"
                data-model="models/wild/hoi_35.glb"
              >
                <img
                  src="models/wild/35_full_image_1.png"
                  class="uk-width-1-1"
                  alt="Example #6"
                >
                <div class="uk-text-meta">Tape-measure</div>
              </a>
            </div>
            <div>
              <a
                class="swap-model"
                href="#"
                data-model="models/oakink/hoi_26.glb"
              >
                <img
                  src="models/oakink/26_cropped_hoi_1.png"
                  class="uk-width-1-1"
                  alt="Example #1"
                >
                <div class="uk-text-meta">Headphones</div>
              </a>
            </div>
            <div>
              <a
                class="swap-model"
                href="#"
                data-model="models/arctic/hoi_960.glb"
              >
                <img
                  src="models/arctic/960_cropped_hoi_0.png"
                  class="uk-width-1-1"
                  alt="Example #2"
                >
                <div class="uk-text-meta">Coffee machine</div>
              </a>
            </div>
            <div>
              <a
                class="swap-model"
                href="#"
                data-model="models/arctic/hoi_325.glb"
              >
                <img
                  src="models/arctic/325_cropped_hoi_1.png"
                  class="uk-width-1-1"
                  alt="Example #3"
                >
                <div class="uk-text-meta">Scissors</div>
              </a>
            </div>
          </div>
        </div>

        <!-- RIGHT: viewer -->
        <div class="uk-width-expand@m">
          <div uk-sticky="offset: 80; bottom: true">
            <model-viewer
              id="fmh-viewer"
              src="models/wild/hoi_4.glb" # Make sure this matches the first example
              camera-controls
              touch-action="pan-y"
              shadow-intensity="0.6"
              exposure="1.0"
              auto-rotate
              poster="images/teaser.png"
              style="width:100%; height:70vh; background:#fff;"
            >
            </model-viewer>
            <div class="uk-text-meta uk-margin-small-top">
              Drag to rotate, scroll/trackpad to zoom.
              Click any example on the left to load its mesh.
            </div>
          </div>
        </div>
      </div>
  - title: "Overview"
    text: |
      <div class="uk-margin">
        <img
          src="images/overview.png"
          alt="Method overview"
          class="uk-width-1-1"
        >
        <div class="uk-text-small uk-text-muted uk-margin-small-top">
          <b>Overview of FollowMyHold.</b> Given a single RGB frame, we (1) isolate the interaction region and derive binary hand/object masks with LangSAM and WiLoR's hand detector; 
          (2) inpaint the occluded object appearance using FLUX.1 Kontext + Gemini (ยง4.1).
          Next, we obtain three complementary 3D cues: a HaMeR hand mesh, a MoGe-2 partial point cloud (with camera pose $\phi$), and a coarse Hunyuan3D-2 HOI mesh. 
          A two-step rigid alignment registers these cues into a common, image-aligned frame.
          A two-step ICP registers all cues into a common image-aligned frame.
          Finally, we perform inference-time guidance with a staged optimization (ยง4.2): Phase~1 optimizes the hand transform $T_h$; 
          Phase~2 optimizes the object transform $T_o$ and guides the velocity field; 
          Phase~3 jointly refines $(T_h,T_o)$ while guiding with pixel-aligned 2D losses ($G_{2\text{D}}$: normals, disparity, silhouette) and 3D constraints ($G_{3\text{D}}$: intersection, proximity).
          The right bottom row shows progressive object refinement over diffusion steps.
        </div>
      </div>
